{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# графики\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "# прогресс-бар\n",
        "from tqdm.auto import tqdm\n",
        "# работа с датасетом CIFAR-10\n",
        "from torchvision.datasets import CIFAR10\n",
        "# трансформации картинок\n",
        "from torchvision.transforms import Compose, Lambda, ToTensor\n",
        "# подготовка батчей данных\n",
        "from torch.utils.data import DataLoader\n",
        "# метрики качества классификации\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-16T07:29:04.360838Z",
          "start_time": "2019-02-16T07:28:56.047198Z"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# необходимая трансформация - датасет возвращает объекты PIL.Image, а нам нужно работать с матрицами\n",
        "# и заодно сразу вытянем картинку в вектор (flatten - превращает матрицу в плоский вектор)\n",
        "transform = Compose([\n",
        "    ToTensor(),\n",
        "    Lambda(lambda x: x.flatten()),\n",
        "])\n",
        "# загрузим датасет (обучающую и тестовую часть)\n",
        "trainset = CIFAR10(root='.', train=True, download=True, transform=transform)\n",
        "testset = CIFAR10(root='.', train=False, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# классы в нашем датасете\n",
        "print(trainset.classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Код однослойного перцептрона (код с семинара с изменениями)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class OneLayerNet:\n",
        "    \n",
        "    def __init__(self, in_d, out_d):\n",
        "        # in_d - размерность входа сети, out_d - размерность выхода сети (количество классов)\n",
        "        # случайная инициализация весов сети\n",
        "        self.W = 1e-3 * np.random.randn(in_d, out_d)\n",
        "        self.bias = np.zeros(out_d)\n",
        "        print(f'Число параметров в сети: {self.W.size + self.bias.size}')\n",
        "    \n",
        "    def softmax(self, scores):\n",
        "        # функция softmax для превращения оценок в вероятности классов\n",
        "        # применяется на последнем слое сети\n",
        "        shift_scores = scores - np.max(scores, axis=1).reshape(-1, 1)\n",
        "        exp_scores = np.exp(shift_scores)\n",
        "        sum_exp_scores = np.sum(exp_scores, axis=1).reshape(-1, 1)\n",
        "        return exp_scores / sum_exp_scores\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        # вычисляет ответ сети для пакета данных, возвращает вероятности классов для каждого входящего примера\n",
        "        # добавили смещение\n",
        "        out = X_batch @ self.W + self.bias\n",
        "        return self.softmax(out)\n",
        "    \n",
        "    def predict(self, X_batch):\n",
        "        # возвращает индекс самого вероятного по мнению сети класса для каждого примера\n",
        "        # а также вероятности классов\n",
        "        y_pred = self.forward(X_batch)\n",
        "        return np.argmax(y_pred, axis=1), y_pred\n",
        "    \n",
        "    def backward(self, X_batch, y_batch, reg):\n",
        "        # рассчитывает значение функции ошибки и ее градиент по весам для применения в градиентном спуске\n",
        "        # словарь, в который положим градиенты по параметрам\n",
        "        grads = {}\n",
        "        batch_size = X_batch.shape[0]\n",
        "        # получаем ответы сети на данном шаге\n",
        "        y_pred = self.forward(X_batch)\n",
        "        # в качестве функции ошибки для задачи классификации используем кросс-энтропию\n",
        "        loss = -np.sum(np.log(y_pred[range(batch_size), y_batch]))\n",
        "        # регуляризация\n",
        "        loss = loss / batch_size + reg * np.sum(self.W ** 2)\n",
        "        # расчет градиента выводится с помощью математики для данного конкретного типа сети\n",
        "        y_pred[range(batch_size), y_batch] -= 1\n",
        "        grads['W'] = X_batch.T @ y_pred\n",
        "        # регуляризация\n",
        "        grads['W'] = grads['W'] / batch_size + 2 * reg * self.W\n",
        "        # bias (смещение)\n",
        "        grads['bias'] = np.mean(y_pred, axis=0)\n",
        "        return loss, grads\n",
        "    \n",
        "    def fit_batch(self, X_batch, y_batch, lr, reg):\n",
        "        # обучение сети на одном батче, градиентный спуск\n",
        "        loss, grads = self.backward(X_batch, y_batch, reg)\n",
        "        # итерация градиентного спуска\n",
        "        self.W -= lr * grads['W']\n",
        "        self.bias -= lr * grads['bias']\n",
        "        return loss"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-16T07:28:42.763981Z",
          "start_time": "2019-02-16T07:28:42.736052Z"
        }
      }
    },
    {
      "source": [
        "## Вспомогательные функции, пригодятся и для двухслойной сети"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# функция применения сети к датасету и подсчета метрик\n",
        "def eval_model(model, dataset, batch_size):\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    # контейнеры для предсказаний сети и правильных меток\n",
        "    predictions, ground_truth = [], []\n",
        "    val_loss = 0\n",
        "    for images_batch, labels_batch in tqdm(dataloader, desc='Evaluation', leave=False):\n",
        "        # конвертация из тензоров в матрицы numpy\n",
        "        images_batch = images_batch.numpy()\n",
        "        labels_batch = labels_batch.numpy()\n",
        "        # у последнего батча может быть несовпадающий размер\n",
        "        bs = images_batch.shape[0]\n",
        "        # предсказания модели\n",
        "        y_pred, y_probs = model.predict(images_batch)\n",
        "        # используем предсказанные вероятности для расчета функции ошибки (кросс-энтропии)\n",
        "        val_loss -= np.sum(np.log(y_probs[range(bs), labels_batch])) / bs\n",
        "        predictions.append(y_pred)\n",
        "        ground_truth.append(labels_batch)\n",
        "    val_loss /= len(dataloader)\n",
        "    predictions = np.concatenate(predictions)\n",
        "    ground_truth = np.concatenate(ground_truth)\n",
        "    # предсказанные метки используем для расчета классификационных метрик\n",
        "    accuracy = accuracy_score(ground_truth, predictions)\n",
        "    report = classification_report(ground_truth, predictions, target_names=dataset.classes)\n",
        "    return val_loss, accuracy, report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# функция обучения сети на датасете\n",
        "def train_model(model, trainset, testset, batch_size, epochs, lr, reg):\n",
        "    train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "    # словарь, в который будем складывать метрики чтобы потом нарисовать графики\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'val_acc': [],\n",
        "    }\n",
        "    # немного другой способ отрисовывания прогресс-баров\n",
        "    with tqdm(total=epochs, desc='Epoch') as epoch_pbar:\n",
        "        for _ in range(epochs):\n",
        "            running_loss = 0\n",
        "            with tqdm(total=len(train_loader), desc='Batch', leave=False) as batch_pbar:\n",
        "                for idx, (images_batch, labels_batch) in enumerate(train_loader):\n",
        "                    # конвертация из тензоров в матрицы numpy\n",
        "                    images_batch = images_batch.numpy()\n",
        "                    labels_batch = labels_batch.numpy()\n",
        "                    loss = model.fit_batch(images_batch, labels_batch, lr, reg)\n",
        "                    # для вывода статистики\n",
        "                    running_loss += loss\n",
        "                    # обновим вложенный прогресс-бар\n",
        "                    batch_pbar.set_postfix(loss=running_loss / (idx + 1))\n",
        "                    batch_pbar.update()\n",
        "            running_loss /= len(train_loader)\n",
        "            # получим метрики на валидации для этой эпохи\n",
        "            val_loss, val_acc, _ = eval_model(model, testset, batch_size)\n",
        "            history['train_loss'].append(running_loss)\n",
        "            history['val_loss'].append(val_loss)\n",
        "            history['val_acc'].append(val_acc)\n",
        "            # обновим прогресс-бар эпох\n",
        "            epoch_pbar.set_postfix(loss=running_loss, val_loss=val_loss, val_acc=val_acc)\n",
        "            epoch_pbar.update()\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_metrics(history):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
        "    n_epochs = len(history['train_loss'])\n",
        "    axes[0].plot(range(n_epochs), history['train_loss'], label='train_loss')\n",
        "    axes[0].plot(range(n_epochs), history['val_loss'], label='val_loss')\n",
        "    axes[0].set_title('Loss')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].legend()\n",
        "\n",
        "    axes[1].plot(range(n_epochs), history['val_acc'], label='val_acc')\n",
        "    axes[1].set_title('Accuracy')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].legend()\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# функция визуализации выученных весов нейронной сети для каждого класса\n",
        "def visualize_weights(weights_matrix, class_names):\n",
        "    weights_matrix = weights_matrix.copy()\n",
        "    # превратим веса относительно каждого класса в набор картинок 32x32x3\n",
        "    weights_matrix = weights_matrix.reshape(3, 32, 32, 10)\n",
        "    # переставим каналы в правильном порядке\n",
        "    weights_matrix = np.moveaxis(weights_matrix, 0, -2)\n",
        "    # для нормировки узнаем минимальный и максимальный веса\n",
        "    w_min, w_max = weights_matrix.min(), weights_matrix.max()\n",
        "    fig, axes = plt.subplots(2, 5, figsize=(16, 8))\n",
        "    axes = [ax for ax_row in axes for ax in ax_row]\n",
        "    for idx, ax in enumerate(axes):\n",
        "        # отмасштабируем веса в отрезок [0, 255]\n",
        "        w_img = 255 * (weights_matrix[:, :, :, idx].squeeze() - w_min) / (w_max - w_min)\n",
        "        w_img = w_img.astype('uint8')\n",
        "        ax.imshow(w_img)\n",
        "        ax.grid(False)\n",
        "        ax.axes.get_xaxis().set_visible(False)\n",
        "        ax.axes.get_yaxis().set_visible(False)\n",
        "        ax.set_title(class_names[idx])\n",
        "    plt.show()"
      ]
    },
    {
      "source": [
        "# Задание 1"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "Вам нужно провести эксперименты по обучению однослойной сети. Постарайтесь получить максимальное качество (accuracy) на тестовой выборке. Экспериментируйте с количеством эпох, batch_size, шагом обучения и коэффициентом регуляризации (reg, дробное число >= 0). По итогам экспериментов зафиксируйте параметры с которыми получилось наилучшее качество и соответствующие графики. Попробуйте описать, как каждый из изменяемых параметров влияет на процесс обучения."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "*Хороший результат для однослойной сети - 40 ~ 45% accuracy*"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model = ...\n",
        "history = ..."
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-16T07:43:29.736345Z",
          "start_time": "2019-02-16T07:42:53.496369Z"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# график метрик\n",
        "plot_metrics(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# финальные метрики\n",
        "_, val_acc, val_report = eval_model(model, testset, batch_size=1024)\n",
        "print(f'Accuracy: {val_acc:.2%}')\n",
        "print(val_report)"
      ]
    },
    {
      "source": [
        "*ваш текст здесь*"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "# Задание 2"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "Возьмите свою лучшую модель из первого задания и визуализируйте ее веса с помощью функции ниже. Попробуйте предположить, что на них можно увидеть."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "visualize_weights(model.W, trainset.classes)"
      ],
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-16T07:47:52.077441Z",
          "start_time": "2019-02-16T07:47:51.657900Z"
        }
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "*ваш текст здесь*"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 3: Двухслойная сеть"
      ],
      "metadata": {}
    },
    {
      "source": [
        "Дополните код в классе двухслойной сети по аналогии с однослойной сетью (для справки используйте материалы лекций и код однослойной сети выше). В качестве функции активации скрытого слоя используйте ReLU. Признаком правильной реализации является постепенное уменьшение train_loss по мере обучения."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoLayerNet:\n",
        "\n",
        "    def __init__(self, in_d, h_d, out_d):\n",
        "        # in_d - размерность входа сети, out_d - размерность выхода сети (количество классов)\n",
        "        # h_d - размерность скрытого слоя сети\n",
        "        # случайная инициализация весов сети\n",
        "        self.W1 = 1e-3 * np.random.randn(in_d, h_d)\n",
        "        self.bias1 = np.zeros(h_d)\n",
        "        self.W2 = 1e-3 * np.random.randn(h_d, out_d)\n",
        "        self.bias2 = np.zeros(out_d)\n",
        "        print(f'Число параметров в сети: {self.W1.size + self.bias1.size + self.W2.size + self.bias2.size}')\n",
        "    \n",
        "    def softmax(self, scores):\n",
        "        # функция softmax для превращения оценок в вероятности классов\n",
        "        # применяется на последнем слое сети\n",
        "        shift_scores = scores - np.max(scores, axis=1).reshape(-1,1)\n",
        "        exp_scores = np.exp(shift_scores)\n",
        "        sum_exp_scores = np.sum(exp_scores, axis=1).reshape(-1, 1)\n",
        "        return exp_scores / sum_exp_scores\n",
        "    \n",
        "    def forward(self, X_batch):\n",
        "        # вычисляет ответ сети для пакета данных, возвращает вероятности классов для каждого входящего примера\n",
        "        # в качестве функции активации скрытого слоя используйте ReLU (подсказка - вам может помочь функция np.maximum)\n",
        "        # h - активации скрытого слоя, они нужны методу backward() для правильного расчета градиента\n",
        "        # ваш код здесь\n",
        "        out1 = ... + self.bias1\n",
        "        # h = ReLU, примененная к out1 - выход первого слоя сети\n",
        "        h = ...\n",
        "        # умножаем h на соответствующие веса и прибавляем bias\n",
        "        out2 = ... + self.bias2\n",
        "        return self.softmax(out2), h\n",
        "    \n",
        "    def predict(self, X_batch):\n",
        "        # метод возвращает индекс самого вероятного по мнению сети класса для каждого примера\n",
        "        # а также вероятности классов\n",
        "        y_pred, _ = self.forward(X_batch)\n",
        "        return np.argmax(y_pred, axis=1), y_pred\n",
        "    \n",
        "    def backward(self, X_batch, y_batch, reg):\n",
        "        # метод рассчитывает значение функции ошибки и ее градиент по весам для применения в градиентном спуске\n",
        "        batch_size = X_batch.shape[0]\n",
        "        # получаем ответы сети на данном шаге и активации скрытого слоя\n",
        "        y_pred, h = self.forward(X_batch)\n",
        "        # в качестве функции ошибки для задачи классификации используем кросс-энтропию\n",
        "        loss = -np.sum(np.log(y_pred[range(batch_size), y_batch])) / batch_size\n",
        "        loss += 0.5 * reg * (np.sum(self.W1 ** 2) + np.sum(self.W2 ** 2))\n",
        "        # расчет градиента выводится с помощью математики для данного конкретного типа сети\n",
        "        # градиенты для каждой матрицы весов положим в словарь\n",
        "        grads = {}\n",
        "        y_pred[range(batch_size), y_batch] -= 1\n",
        "        grads['W2'] = h.T @ y_pred\n",
        "        grads['W2'] = grads['W2'] / batch_size + reg * self.W2\n",
        "        grads['bias2'] = np.mean(y_pred, axis=0)\n",
        "\n",
        "        dh = y_pred @ self.W2.T\n",
        "        dh_relu = (h > 0) * dh\n",
        "        grads['W1'] = X_batch.T @ dh_relu \n",
        "        grads['W1'] = grads['W1'] / batch_size + reg * self.W1\n",
        "        grads['bias1'] = np.mean(dh_relu, axis=0)\n",
        "        return loss, grads\n",
        "    \n",
        "    def fit_batch(self, X_batch, y_batch, lr, reg):\n",
        "        # обучение сети на одном батче, градиентный спуск\n",
        "        loss, grads = self.backward(X_batch, y_batch, reg)\n",
        "        # итерация градиентного спуска\n",
        "        # ваш код здесь\n",
        "        self.W1 = ...\n",
        "        self.bias1 = ...\n",
        "        self.W2 = ...\n",
        "        self.bias2 = ...\n",
        "        return loss"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-02-15T21:23:46.118320Z",
          "start_time": "2019-02-15T21:23:46.087196Z"
        }
      }
    },
    {
      "source": [
        "# Задание 4"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "Теперь проведите эксприменты с двухслойной сетью. У вас появляется новый параметр - количество нейронов в скрытом слое. Замечание: параметры, подобранные для однослойной сети не будут оптимальными для двухслойной. Если ваша сеть сильно переобучается - либо уменьшайте количество нейронов, либо повышайте параметр reg, если сеть практически не обучается - попробуйте увеличить learning rate или уменьшить reg.\n",
        "\n",
        "Опишите результаты экспериментов и зафиксируйте параметры и графики лучшей модели."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = ...\n",
        "history = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_metrics(history)"
      ]
    },
    {
      "source": [
        "*Для хорошо обученной двухслойной сети можно получить accuracy > 50%. При этом двухслойная сеть может сходиться сильно медленнее однослойной.*"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "source": [
        "*ваш текст здесь*"
      ],
      "cell_type": "markdown",
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python392jvsc74a57bd0b535a82a3b9259ae693695d68fa86b08ae8b3bec72aa6a0359cf505f9926a8ce",
      "display_name": "Python 3.9.2 64-bit ('nn179': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.2-final",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "0.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}